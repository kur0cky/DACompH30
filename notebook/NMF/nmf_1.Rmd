---
title: "nmf_1"
author: "Yutaka Kuroki"
date: "2018/11/5"
output:
  html_document:
    number_sections: true
    fig_caption: true
    toc: true
    toc_float: true
    toc_depth: 2
    fig_width: 7
    theme: cosmo
    highlight: tango
    code_folding: hide
editor_options: 
  chunk_output_type: console
md_extensions: -ascii_identifiers
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      message = FALSE,
                      warning = FALSE,
                      cache = TRUE)
```

```{r import, cache=FALSE}
library(tidyverse)
library(lubridate)
library(dbplyr)
library(dbplot)
library(RPostgreSQL)
library(ggforce)
library(DT)
library(NMF)
library(knitr)

theme_set(theme_bw(base_family = "HiraKakuPro-W3"))
```

```{r connect, cache=FALSE}
conn <- dbConnect(PostgreSQL(),
                  host = scan("connection.txt", what = character())[1],
                  port = scan("connection.txt", what = character())[2], 
                  dbname = scan("connection.txt", what = character())[3], 
                  user = scan("connection.txt", what = character())[4], 
                  password = scan("connection.txt", what = character())[5])

orgn <- conn %>% 
  tbl(from = dbplyr::in_schema("edit", "tv_orgn_program_2")) 
tmp <- conn %>% 
  tbl(from = dbplyr::in_schema("processed", "tv_orgn_p_cv")) 
program <- conn %>% 
  tbl(from = dbplyr::in_schema("processed", "tv_program")) 
ban <- conn %>% 
  tbl(from = dbplyr::in_schema("processed", "bancluster_mst")) 
ban1 <- conn %>% 
  tbl(from = dbplyr::in_schema("sub_mst", "ban_code1_mst")) 
prof_data <- conn %>% 
  tbl(from = dbplyr::in_schema("processed", "profiledata"))
prof_mst <- conn %>% 
  tbl(from = dbplyr::in_schema("processed", "profilemaster"))
prof <- prof_data %>% 
  left_join(prof_mst, by = c("qu_genre_code", "question_code", "answer_code"))

res3 <- read_rds("data/result_nmf_3.rds")
res5 <- read_rds("data/result_nmf_5.rds")
res7 <- read_rds("data/result_nmf_7.rds")
res9 <- read_rds("data/result_nmf_9.rds")
```

# はじめに

テレビ視聴データを用いるにあたって

- テレビ番組を**語句**
- 視聴者を**書き手**

とみなすことによって，テキスト分析の手法や解釈が持ってこれると考えている（勝手に）  
（係り受けに由来するようなものは除く）

# 語句の重み

情報検査，コーパス分析，文章の自動要約などを行うとき，語句（今回の場合はテレビ番組）の重要度を示す重みが必要な場合がある．
**重みの値が大きいほど，その語句（番組）が該当するテキストのキーワードになることを意味する．**
語句の重みに関する指標は多く提案されている

**ブーリアン重み付け**  
対象語句がテキストの中に現れれば1，そうでなければ0

**頻度重み付け**  
対象語句がテキストの中に現れた回数(TF: term frequency)．  
長さが異なる複数のテキストについて分析するときは長さの影響を取り除くため，テキストの長さで調整を行った調整頻度(rTF: relative term frequency)を用いることが多い（今回の分析でもそうした）．
また，対数をとる場合もある

**TF-IDF重み付け**  
$$\text{TF-IDF} = TF * IDF$$  
で定義される．ここでIDF(inverted document frequency)は，語句がどのくらいのテキストに現れているかに関する度合いである．最もシンプルなIDFは次で定義される  
$$\text{IDF}_i = {\rm log}\left(\frac{N}{df_i}\right)$$  
式の中の$df_i$は対象語句$_i$を含むテキストの数，$N$はテキストの総数である．  
IDFを導入するメリットは，汎用的な単語に対するフィルターである．**いくつもの文書で横断的に使われている単語はそんなに重要じゃない**という発想に由来する．

**エントロピー重み付け**  
IDFを単語（番組）がテキスト（視聴者）に現れる（視聴される）確率ととらえ，シャノンのエントロピーによる重み付けを考えることができる

# テレビ視聴におけるTF-IDF

本分析では，テレビ視聴に対するTF-IDFを定義する．テレビ番組集合を$i=\{1,\dots,M\}$とし，視聴者集合を$j=\{1,\dots,N\}$とする．

$$
\text{TF-IDF}_{i,j} = \text{TF}_{i,j}\times\text{IDF}_{i} =\\
\frac{w_{i,j}}{\sum_{i}w_{i,j}} \times {\rm log}\left(\frac{N}{df_{i}}\right)
$$

# NMF（非負値行列因子分解）

NMFは得られた非負行列を以下のように二つの行列に分解することを前提としている．

＃使用データ

```{r}
program %>% 
  group_by(program_code) %>% 
  summarise(sum = sum(program_time)) %>% 
  arrange(desc(sum)) %>% 
  ggplot(aes(sum))+
  geom_density()+
  scale_x_continuous(limits = c(0,10000))
```


# データ解析

- KL距離ベースの推定
- 計算時間（並列化なし）
    - $k=3$ : 35分
    - $k=5$ : 44分
    - $k=7$ : 60分
- 残差
    - $k=3$ : 15768
    - $k=5$ : 14606
    - $k=7$ : 13884



## TOP20{.tabset}


```{r pro_mst}
pro_mst <- program %>% 
  count(station_code, program_code, program_name) %>% 
  collect() %>% 
  arrange(program_code, desc(n)) %>% 
  group_by(program_code) %>% 
  slice(1) %>% 
  mutate(program_name = str_remove_all(program_name, pattern = " "),
         program_name = str_c(station_code, ".", program_name)) %>% 
  select(-station_code) %>% 
  filter(n != 1)
```


### 基底ベクトル3個

- 流石に3個は少なすぎたか
- なんとなく分かれてそうではある

```{r res3_top20}
basis3 <- basis(res3)
basis3 %>% 
  as.data.frame() %>% 
  rownames_to_column(var = "program_code") %>% 
  as_tibble() %>% 
  gather(topic, value, -program_code) %>% 
  left_join(pro_mst, by = "program_code") %>% 
  group_by(topic) %>% 
  mutate(rank = min_rank(desc(value))) %>% 
  filter(rank < 31) %>% 
  select( -program_code, -n, -value) %>% 
  spread(topic, program_name) %>% 
  kable()
```


### 基底ベクトル5個

- トピック1とトピック3が綺麗に抜けている
- 他のトピックも情報番組の放送局など，傾向はあるが，何個かのトピックが混ざっている
- 

```{r }
basis5 <- basis(res5)
basis5 %>% 
  as.data.frame() %>% 
  rownames_to_column(var = "program_code") %>% 
  as_tibble() %>% 
  gather(topic, value, -program_code) %>% 
  right_join(pro_mst, by = "program_code") %>% 
  group_by(topic) %>% 
  mutate(rank = min_rank(desc(value))) %>% 
  filter(rank < 21) %>% 
  select( -program_code, -n, -value) %>% 
  spread(topic, program_name) 
```


```{r}
a <- basis5 %>% 
  as.data.frame() %>% 
  rownames_to_column(var = "program_code") %>% 
  as_tibble() %>% 
  gather(topic, value, -program_code) %>% 
  right_join(pro_mst, by = "program_code") %>% 
  group_by(topic) %>% 
  mutate(rank = min_rank(desc(value))) %>% 
  filter(rank < 101, topic == "V2") 

par(family = "Osaka")
wordcloud(a$program_name, a$value, random.order = F)
```

### 基底ベクトル7個

- トピック1とトピック3が綺麗に抜けている
- 他のトピックも情報番組の放送局など，傾向はあるが，何個かのトピックが混ざっている
- 

```{r }
basis7 <- basis(res7)
basis7 %>% 
  as.data.frame() %>% 
  rownames_to_column(var = "program_code") %>% 
  as_tibble() %>% 
  gather(topic, value, -program_code) %>% 
  right_join(pro_mst, by = "program_code") %>% 
  group_by(topic) %>% 
  mutate(rank = min_rank(desc(value))) %>% 
  filter(rank < 21) %>% 
  select( -program_code, -n, -value) %>% 
  spread(topic, program_name) %>% View()
```


```{r}
a <- basis5 %>% 
  as.data.frame() %>% 
  rownames_to_column(var = "program_code") %>% 
  as_tibble() %>% 
  gather(topic, value, -program_code) %>% 
  right_join(pro_mst, by = "program_code") %>% 
  group_by(topic) %>% 
  mutate(rank = min_rank(desc(value))) %>% 
  filter(rank < 101, topic == "V2") 

par(family = "Osaka")
wordcloud(a$program_name, a$value, random.order = F)
```


### 基底ベクトル9個

- トピック1とトピック3が綺麗に抜けている
- 他のトピックも情報番組の放送局など，傾向はあるが，何個かのトピックが混ざっている
- 

```{r}
basis9 <- basis(res9)
basis9 %>% 
  as.data.frame() %>% 
  rownames_to_column(var = "program_code") %>% 
  as_tibble() %>% 
  gather(topic, value, -program_code) %>% 
  right_join(pro_mst, by = "program_code") %>% 
  group_by(topic) %>% 
  mutate(rank = min_rank(desc(value))) %>% 
  filter(rank < 21) %>% 
  select( -program_code, -n, -value) %>% 
  spread(topic, program_name) %>% View()
```


```{r}
a <- basis5 %>% 
  as.data.frame() %>% 
  rownames_to_column(var = "program_code") %>% 
  as_tibble() %>% 
  gather(topic, value, -program_code) %>% 
  right_join(pro_mst, by = "program_code") %>% 
  group_by(topic) %>% 
  mutate(rank = min_rank(desc(value))) %>% 
  filter(rank < 101, topic == "V2") 

par(family = "Osaka")
wordcloud(a$program_name, a$value, random.order = F)
```

##


```{r}
innov <- prof %>% 
  filter(qu_genre_code == 25, question_code == 6) %>% 
  select(house_num, answer, answer_code) %>% 
  collect() %>% 
  mutate(answer = str_remove_all(answer, pattern = " "))

coef5 <- coef(res5)
apply(coef5, 2, function(x) which(x == max(x)) ) %>% 
  data.frame(topic = .) %>% 
  rownames_to_column(var = "house_num") %>% 
  as_tibble() %>% 
  mutate(house_num = as.integer(house_num)) %>% 
  left_join(innov, by = "house_num") %>% 
  replace_na(list(answer = "欠損", answer_code = 10)) %>% 
  count(topic, answer_code) %>% 
  group_by(topic) %>% 
  mutate(rate = n / sum(n)) %>% 
  ggplot(aes(topic, rate, fill = factor(answer_code)))+
  geom_bar(stat = "identity")+
  scale_fill_viridis_d()
  
  
```

